{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 101 - Linear Model on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will program simple Keras examples using the Sequential API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt # For plotting purposes\n",
    "\n",
    "\n",
    "# Bug still not solved, resort to old keras module (see below)\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Use the above import when the bug is solved\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "# ------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension (for visualization purposes)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each MNIST image has shape (28,28)\n",
    "mnist_img_rows, mnist_img_cols = 28, 28\n",
    "dim_target = 10\n",
    "\n",
    "# Load the MNIST dataset, which has already been splitted into training and test set\n",
    "(mnist_train, mnist_y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the shape of our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 examples of (28, 28) features each value\n",
      "Training targets have shape (60000,) i.e. we will predict a constant target value for each example\n",
      "\n",
      "Test set has 10000 examples of (28, 28) features each value\n",
      "Test targets have shape (10000,) i.e. we will predict a constant target value for each example\n",
      "\n",
      "Examples have type uint8, targets have type uint8\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set has {mnist_train.shape[0]} examples of {mnist_train.shape[1:]} features each value')\n",
    "print(f'Training targets have shape {mnist_y_train.shape} i.e. we will predict a constant target value for each example')\n",
    "print('')\n",
    "print(f'Test set has {x_test.shape[0]} examples of {x_test.shape[1:]} features each value')\n",
    "print(f'Test targets have shape {y_test.shape} i.e. we will predict a constant target value for each example')\n",
    "print('')\n",
    "print(f'Examples have type {mnist_train.dtype}, targets have type {mnist_y_train.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a validation set from the training set (the Pythonic way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use scikit-learn when working with NumPy matrices. Use the ``sklearn.model_selection.train_test_split`` function (that you can use to extract a train/test split from the whole dataset and a validation set from the train set) or the ``sklearn.model_selection.KFold`` function for k-fold (external or internal) cross validation. **Remember: external cross validation is used for model assessment (average of k performances on unseen TEST data), whereas internal cross validation is used for model selection (average of k performances on VALIDATION data to select the best configuration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subset(x_dataset, y_dataset, subset_percentage=10):\n",
    "    # Choose a seed to obtain the same split\n",
    "    random.seed(42)\n",
    "    \n",
    "    no_samples = x_dataset.shape[0]\n",
    "\n",
    "    # Generate a list of indices\n",
    "    indices = [i for i in range(no_samples)]\n",
    "\n",
    "    # We want the x% of the training set to be used as a validation set\n",
    "    no_subset_samples = int((no_samples/100)*subset_percentage)\n",
    "\n",
    "    # In place shuffling of the data structure\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Display the indices\n",
    "    # print(f'Validation indices are: {indices[:no_subset_samples]}')\n",
    "\n",
    "    # The subset will correspond to the first \"no_subset_samples\" shuffled indices\n",
    "    x_subset = x_dataset[indices[:no_subset_samples], :]\n",
    "    y_subset = y_dataset[indices[:no_subset_samples]]\n",
    "\n",
    "    # The rest of the data will correspond to other shuffled indices\n",
    "    new_x_dataset = x_dataset[indices[no_subset_samples:], :]\n",
    "    new_y_dataset = y_dataset[indices[no_subset_samples:]]\n",
    "    \n",
    "    return (new_x_dataset, new_y_dataset), (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a validation set from the TRAINING set.\n",
    "# You can also use this function to split you entire dataset into training, validation and test sets when these are not given.\n",
    "(x_train, y_train), (x_valid, y_valid) = extract_subset(mnist_train, mnist_y_train, subset_percentage=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], -1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The associated target label is 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape(mnist_img_rows, mnist_img_cols), cmap='gray')\n",
    "print(f'The associated target label is {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First define the hyper-parameters of the model\n",
    "### Later, we can try to do model selection via random search on hyper-parameters ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of the model\n",
    "l1_regularization = 0.  # L1 regularization to be applied to the weight matrix W\n",
    "l2_regularization = 1e-5  # L2 regularization to be applied to the weight matrix W\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "batch_size = 64  # Mini-batch training for stochastic gradient descent\n",
    "no_epochs = 20  # Maximum number of epochs to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a linear model with the Sequential API\n",
    "linear = Sequential()\n",
    "\n",
    "# Let's add the connection between input and output\n",
    "linear.add(Dense(units=dim_target,  # depends on the task at hand\n",
    "                 activation='linear',  # the identiy mapping (no need to use non-linearities here, it's a linear model!)\n",
    "                 use_bias=True,  # we want to implement Wx + b where b is the bias\n",
    "                 kernel_regularizer=regularizers.l1_l2(l1=l1_regularization, l2=l2_regularization)  # Regularizes the weight matrix W                 \n",
    "                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.compile(optimizer=optimizers.SGD(lr=learning_rate),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),  # Sparse will convert categorical labels for us in one-hot form!\n",
    "              # List of metrics to monitor\n",
    "              metrics=[metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "52608/54000 [============================>.] - ETA: 0s - loss: 2.0157 - sparse_categorical_accuracy: 0.3591\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.59650, saving model to checkpoints/linear/cp.ckpt\n",
      "WARNING:tensorflow:From /home/errica/anaconda3/envs/intro-keras/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 2.0073 - sparse_categorical_accuracy: 0.3654 - val_loss: 1.6956 - val_sparse_categorical_accuracy: 0.5965\n",
      "Epoch 2/20\n",
      "53632/54000 [============================>.] - ETA: 0s - loss: 1.4880 - sparse_categorical_accuracy: 0.6914\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.59650 to 0.74183, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 1.4867 - sparse_categorical_accuracy: 0.6922 - val_loss: 1.3199 - val_sparse_categorical_accuracy: 0.7418\n",
      "Epoch 3/20\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 1.2009 - sparse_categorical_accuracy: 0.7723\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.74183 to 0.78800, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 1.2006 - sparse_categorical_accuracy: 0.7724 - val_loss: 1.1047 - val_sparse_categorical_accuracy: 0.7880\n",
      "Epoch 4/20\n",
      "53120/54000 [============================>.] - ETA: 0s - loss: 1.0304 - sparse_categorical_accuracy: 0.8015\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.78800 to 0.81283, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 1.0293 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.9698 - val_sparse_categorical_accuracy: 0.8128\n",
      "Epoch 5/20\n",
      "53952/54000 [============================>.] - ETA: 0s - loss: 0.9171 - sparse_categorical_accuracy: 0.8171\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.81283 to 0.82683, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.9173 - sparse_categorical_accuracy: 0.8171 - val_loss: 0.8774 - val_sparse_categorical_accuracy: 0.8268\n",
      "Epoch 6/20\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.8385 - sparse_categorical_accuracy: 0.8277\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.82683 to 0.83567, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.8385 - sparse_categorical_accuracy: 0.8277 - val_loss: 0.8106 - val_sparse_categorical_accuracy: 0.8357\n",
      "Epoch 7/20\n",
      "52864/54000 [============================>.] - ETA: 0s - loss: 0.7806 - sparse_categorical_accuracy: 0.8354\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.83567 to 0.84283, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.7799 - sparse_categorical_accuracy: 0.8354 - val_loss: 0.7599 - val_sparse_categorical_accuracy: 0.8428\n",
      "Epoch 8/20\n",
      "53824/54000 [============================>.] - ETA: 0s - loss: 0.7345 - sparse_categorical_accuracy: 0.8412\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.84283 to 0.85167, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.8412 - val_loss: 0.7198 - val_sparse_categorical_accuracy: 0.8517\n",
      "Epoch 9/20\n",
      "53696/54000 [============================>.] - ETA: 0s - loss: 0.6982 - sparse_categorical_accuracy: 0.8465\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.85167 to 0.85383, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.6981 - sparse_categorical_accuracy: 0.8466 - val_loss: 0.6873 - val_sparse_categorical_accuracy: 0.8538\n",
      "Epoch 10/20\n",
      "53760/54000 [============================>.] - ETA: 0s - loss: 0.6685 - sparse_categorical_accuracy: 0.8502\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.85383 to 0.85550, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.6683 - sparse_categorical_accuracy: 0.8502 - val_loss: 0.6604 - val_sparse_categorical_accuracy: 0.8555\n",
      "Epoch 11/20\n",
      "52416/54000 [============================>.] - ETA: 0s - loss: 0.6430 - sparse_categorical_accuracy: 0.8536\n",
      "Epoch 00011: val_sparse_categorical_accuracy improved from 0.85550 to 0.85750, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.6434 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.6377 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 12/20\n",
      "53312/54000 [============================>.] - ETA: 0s - loss: 0.6221 - sparse_categorical_accuracy: 0.8561\n",
      "Epoch 00012: val_sparse_categorical_accuracy improved from 0.85750 to 0.85917, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.6221 - sparse_categorical_accuracy: 0.8562 - val_loss: 0.6182 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 13/20\n",
      "52928/54000 [============================>.] - ETA: 0s - loss: 0.6040 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 00013: val_sparse_categorical_accuracy improved from 0.85917 to 0.86233, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.6038 - sparse_categorical_accuracy: 0.8583 - val_loss: 0.6013 - val_sparse_categorical_accuracy: 0.8623\n",
      "Epoch 14/20\n",
      "52992/54000 [============================>.] - ETA: 0s - loss: 0.5879 - sparse_categorical_accuracy: 0.8602\n",
      "Epoch 00014: val_sparse_categorical_accuracy improved from 0.86233 to 0.86367, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.5877 - sparse_categorical_accuracy: 0.8603 - val_loss: 0.5864 - val_sparse_categorical_accuracy: 0.8637\n",
      "Epoch 15/20\n",
      "53376/54000 [============================>.] - ETA: 0s - loss: 0.5741 - sparse_categorical_accuracy: 0.8622\n",
      "Epoch 00015: val_sparse_categorical_accuracy improved from 0.86367 to 0.86533, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5735 - sparse_categorical_accuracy: 0.8623 - val_loss: 0.5732 - val_sparse_categorical_accuracy: 0.8653\n",
      "Epoch 16/20\n",
      "52736/54000 [============================>.] - ETA: 0s - loss: 0.5613 - sparse_categorical_accuracy: 0.8644\n",
      "Epoch 00016: val_sparse_categorical_accuracy improved from 0.86533 to 0.86633, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5609 - sparse_categorical_accuracy: 0.8645 - val_loss: 0.5614 - val_sparse_categorical_accuracy: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "53184/54000 [============================>.] - ETA: 0s - loss: 0.5496 - sparse_categorical_accuracy: 0.8662\n",
      "Epoch 00017: val_sparse_categorical_accuracy improved from 0.86633 to 0.86783, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5496 - sparse_categorical_accuracy: 0.8662 - val_loss: 0.5508 - val_sparse_categorical_accuracy: 0.8678\n",
      "Epoch 18/20\n",
      "52800/54000 [============================>.] - ETA: 0s - loss: 0.5392 - sparse_categorical_accuracy: 0.8678\n",
      "Epoch 00018: val_sparse_categorical_accuracy improved from 0.86783 to 0.86933, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5394 - sparse_categorical_accuracy: 0.8676 - val_loss: 0.5412 - val_sparse_categorical_accuracy: 0.8693\n",
      "Epoch 19/20\n",
      "52544/54000 [============================>.] - ETA: 0s - loss: 0.5307 - sparse_categorical_accuracy: 0.8689\n",
      "Epoch 00019: val_sparse_categorical_accuracy improved from 0.86933 to 0.86983, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.5301 - sparse_categorical_accuracy: 0.8691 - val_loss: 0.5324 - val_sparse_categorical_accuracy: 0.8698\n",
      "Epoch 20/20\n",
      "52864/54000 [============================>.] - ETA: 0s - loss: 0.5216 - sparse_categorical_accuracy: 0.8703\n",
      "Epoch 00020: val_sparse_categorical_accuracy improved from 0.86983 to 0.87083, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.5216 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.5244 - val_sparse_categorical_accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_linear\n",
    "\n",
    "# Set up a log folder in which we will store the output to be displayed on TensorBoard\n",
    "logdir = \"logs_linear/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Chekpoint path for storing our model\n",
    "checkpoint_path = \"checkpoints/linear/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# There are many possible ways to store a model, for example every n epochs. Check the documentation, very useful!\n",
    "save_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,  # Save only the best epoch according to valid set\n",
    "                                                 monitor=\"val_sparse_categorical_accuracy\",  # Save according to valid acc\n",
    "                                                 verbose=1,\n",
    "                                                 save_weights_only=False,  # we want to save the graph as well  \n",
    "                                                 )  \n",
    "\n",
    "# Train the Model!\n",
    "# Note: fit has also the chance to specify a validation split percentage\n",
    "print('# Fit model on training data')\n",
    "history = linear.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=no_epochs,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[save_callback, tensorboard_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6db877f210820a86\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6db877f210820a86\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize your model\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model from disk (for demo purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time! \n",
    "### We cannot change the hyper-parameters once we evaluate on test!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.49705806546211245, Test Accuracy: 87.91000247001648\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)  # Verbose 0 does not show any log\n",
    "print(f'Test loss: {score[0]}, Test Accuracy: {score[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## We found the TEST accuracy of our linear model. There's no turning back now, you should go to your boss to report this result.\n",
    "## If you start again changing hyper-parameters and see if it improves on TEST, that is dead wrong! You are using the TEST as a VALIDATION !!\n",
    "\n",
    "### Instead, take some more time to validate a broader range of configurations on the VALIDATION test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's classify one example from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe8cdcc71d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape(mnist_img_rows, mnist_img_cols), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class that has maximum probability associated to the input is: 7\n"
     ]
    }
   ],
   "source": [
    "# Add a softmax activation to get probabilities\n",
    "model.add(Softmax())\n",
    "\n",
    "# Predict output\n",
    "y = model.predict(np.expand_dims(x_test[0,:], 0))\n",
    "\n",
    "print(f'The class that has maximum probability associated to the input is: {np.argmax(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exercise time! Change the code below to train an MLP!\n",
    "## I have included a complete example with the code to perform model selection via random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1lRjIAnAV9kPNMWPOcF3ub4odChbTkFwR\" max-heigth=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/25\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 1.5150 - sparse_categorical_accuracy: 0.6256 - val_loss: 1.0551 - val_sparse_categorical_accuracy: 0.7928\n",
      "Epoch 2/25\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.8937 - sparse_categorical_accuracy: 0.8158 - val_loss: 0.7804 - val_sparse_categorical_accuracy: 0.8368\n",
      "Epoch 3/25\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.7147 - sparse_categorical_accuracy: 0.8418 - val_loss: 0.6662 - val_sparse_categorical_accuracy: 0.8505\n",
      "Epoch 4/25\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.6281 - sparse_categorical_accuracy: 0.8537 - val_loss: 0.6022 - val_sparse_categorical_accuracy: 0.8577\n",
      "Epoch 5/25\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5757 - sparse_categorical_accuracy: 0.8615 - val_loss: 0.5607 - val_sparse_categorical_accuracy: 0.8653\n",
      "Epoch 6/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5399 - sparse_categorical_accuracy: 0.8678 - val_loss: 0.5312 - val_sparse_categorical_accuracy: 0.8693\n",
      "Epoch 7/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5137 - sparse_categorical_accuracy: 0.8722 - val_loss: 0.5091 - val_sparse_categorical_accuracy: 0.8718\n",
      "Epoch 8/25\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4936 - sparse_categorical_accuracy: 0.8758 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8748\n",
      "Epoch 9/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4774 - sparse_categorical_accuracy: 0.8785 - val_loss: 0.4773 - val_sparse_categorical_accuracy: 0.8765\n",
      "Epoch 10/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4641 - sparse_categorical_accuracy: 0.8810 - val_loss: 0.4654 - val_sparse_categorical_accuracy: 0.8803\n",
      "Epoch 11/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4530 - sparse_categorical_accuracy: 0.8834 - val_loss: 0.4556 - val_sparse_categorical_accuracy: 0.8795\n",
      "Epoch 12/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4434 - sparse_categorical_accuracy: 0.8852 - val_loss: 0.4468 - val_sparse_categorical_accuracy: 0.8808\n",
      "Epoch 13/25\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4352 - sparse_categorical_accuracy: 0.8868 - val_loss: 0.4394 - val_sparse_categorical_accuracy: 0.8825\n",
      "Epoch 14/25\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4279 - sparse_categorical_accuracy: 0.8887 - val_loss: 0.4330 - val_sparse_categorical_accuracy: 0.8828\n",
      "Epoch 15/25\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4215 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.4272 - val_sparse_categorical_accuracy: 0.8863\n",
      "Epoch 16/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4156 - sparse_categorical_accuracy: 0.8914 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8875\n",
      "Epoch 17/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4105 - sparse_categorical_accuracy: 0.8926 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8882\n",
      "Epoch 18/25\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4058 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.4130 - val_sparse_categorical_accuracy: 0.8882\n",
      "Epoch 19/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4015 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.4092 - val_sparse_categorical_accuracy: 0.8895\n",
      "Epoch 20/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3975 - sparse_categorical_accuracy: 0.8952 - val_loss: 0.4056 - val_sparse_categorical_accuracy: 0.8907\n",
      "Epoch 21/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3939 - sparse_categorical_accuracy: 0.8960 - val_loss: 0.4024 - val_sparse_categorical_accuracy: 0.8910\n",
      "Epoch 22/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3906 - sparse_categorical_accuracy: 0.8967 - val_loss: 0.3993 - val_sparse_categorical_accuracy: 0.8917\n",
      "Epoch 23/25\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3875 - sparse_categorical_accuracy: 0.8974 - val_loss: 0.3966 - val_sparse_categorical_accuracy: 0.8925\n",
      "Epoch 24/25\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3845 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.3938 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 25/25\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3818 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.3915 - val_sparse_categorical_accuracy: 0.8935\n",
      "Validation loss: 0.39146446299552917, Validation Accuracy: 89.34999704360962\n",
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/44\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 1.5285 - sparse_categorical_accuracy: 0.6239 - val_loss: 1.0818 - val_sparse_categorical_accuracy: 0.7775\n",
      "Epoch 2/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.9169 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.8072 - val_sparse_categorical_accuracy: 0.8297\n",
      "Epoch 3/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7381 - sparse_categorical_accuracy: 0.8389 - val_loss: 0.6924 - val_sparse_categorical_accuracy: 0.8497\n",
      "Epoch 4/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.6514 - sparse_categorical_accuracy: 0.8518 - val_loss: 0.6282 - val_sparse_categorical_accuracy: 0.8583\n",
      "Epoch 5/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5991 - sparse_categorical_accuracy: 0.8600 - val_loss: 0.5867 - val_sparse_categorical_accuracy: 0.8632\n",
      "Epoch 6/44\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5637 - sparse_categorical_accuracy: 0.8660 - val_loss: 0.5575 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 7/44\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5379 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5354 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 8/44\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5182 - sparse_categorical_accuracy: 0.8737 - val_loss: 0.5181 - val_sparse_categorical_accuracy: 0.8717\n",
      "Epoch 9/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5024 - sparse_categorical_accuracy: 0.8772 - val_loss: 0.5044 - val_sparse_categorical_accuracy: 0.8742\n",
      "Epoch 10/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4895 - sparse_categorical_accuracy: 0.8800 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8758\n",
      "Epoch 11/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4787 - sparse_categorical_accuracy: 0.8823 - val_loss: 0.4833 - val_sparse_categorical_accuracy: 0.8773\n",
      "Epoch 12/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4696 - sparse_categorical_accuracy: 0.8842 - val_loss: 0.4748 - val_sparse_categorical_accuracy: 0.8788\n",
      "Epoch 13/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4616 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.4677 - val_sparse_categorical_accuracy: 0.8818\n",
      "Epoch 14/44\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.4547 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.8835\n",
      "Epoch 15/44\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.4486 - sparse_categorical_accuracy: 0.8886 - val_loss: 0.4556 - val_sparse_categorical_accuracy: 0.8852\n",
      "Epoch 16/44\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.4431 - sparse_categorical_accuracy: 0.8896 - val_loss: 0.4508 - val_sparse_categorical_accuracy: 0.8872\n",
      "Epoch 17/44\n",
      "54000/54000 [==============================] - 1s 19us/sample - loss: 0.4383 - sparse_categorical_accuracy: 0.8907 - val_loss: 0.4463 - val_sparse_categorical_accuracy: 0.8877\n",
      "Epoch 18/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4338 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.4423 - val_sparse_categorical_accuracy: 0.8888\n",
      "Epoch 19/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4298 - sparse_categorical_accuracy: 0.8926 - val_loss: 0.4387 - val_sparse_categorical_accuracy: 0.8893\n",
      "Epoch 20/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4261 - sparse_categorical_accuracy: 0.8934 - val_loss: 0.4352 - val_sparse_categorical_accuracy: 0.8910\n",
      "Epoch 21/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4227 - sparse_categorical_accuracy: 0.8944 - val_loss: 0.4324 - val_sparse_categorical_accuracy: 0.8918\n",
      "Epoch 22/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4196 - sparse_categorical_accuracy: 0.8952 - val_loss: 0.4293 - val_sparse_categorical_accuracy: 0.8932\n",
      "Epoch 23/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4167 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.4267 - val_sparse_categorical_accuracy: 0.8930\n",
      "Epoch 24/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4141 - sparse_categorical_accuracy: 0.8964 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8935\n",
      "Epoch 25/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4116 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8948\n",
      "Epoch 26/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4092 - sparse_categorical_accuracy: 0.8976 - val_loss: 0.4200 - val_sparse_categorical_accuracy: 0.8955\n",
      "Epoch 27/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4070 - sparse_categorical_accuracy: 0.8984 - val_loss: 0.4179 - val_sparse_categorical_accuracy: 0.8960\n",
      "Epoch 28/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4050 - sparse_categorical_accuracy: 0.8987 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8965\n",
      "Epoch 29/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4031 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4144 - val_sparse_categorical_accuracy: 0.8967\n",
      "Epoch 30/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4013 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4128 - val_sparse_categorical_accuracy: 0.8968\n",
      "Epoch 31/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3995 - sparse_categorical_accuracy: 0.9001 - val_loss: 0.4112 - val_sparse_categorical_accuracy: 0.8975\n",
      "Epoch 32/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3979 - sparse_categorical_accuracy: 0.9008 - val_loss: 0.4099 - val_sparse_categorical_accuracy: 0.8978\n",
      "Epoch 33/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3964 - sparse_categorical_accuracy: 0.9015 - val_loss: 0.4083 - val_sparse_categorical_accuracy: 0.8975\n",
      "Epoch 34/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3949 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.4070 - val_sparse_categorical_accuracy: 0.8982\n",
      "Epoch 35/44\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.3935 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8983\n",
      "Epoch 36/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3922 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.4046 - val_sparse_categorical_accuracy: 0.8987\n",
      "Epoch 37/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3909 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.4035 - val_sparse_categorical_accuracy: 0.8987\n",
      "Epoch 38/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3897 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.4025 - val_sparse_categorical_accuracy: 0.8995\n",
      "Epoch 39/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3885 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.4015 - val_sparse_categorical_accuracy: 0.8987\n",
      "Epoch 40/44\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3874 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.4005 - val_sparse_categorical_accuracy: 0.8998\n",
      "Epoch 41/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3864 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3995 - val_sparse_categorical_accuracy: 0.8987\n",
      "Epoch 42/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3854 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3986 - val_sparse_categorical_accuracy: 0.9005\n",
      "Epoch 43/44\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3844 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.3979 - val_sparse_categorical_accuracy: 0.9002\n",
      "Epoch 44/44\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3835 - sparse_categorical_accuracy: 0.9057 - val_loss: 0.3970 - val_sparse_categorical_accuracy: 0.9002\n",
      "Validation loss: 0.3969514450232188, Validation Accuracy: 90.01666903495789\n",
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/43\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 1.6116 - sparse_categorical_accuracy: 0.5862 - val_loss: 1.1804 - val_sparse_categorical_accuracy: 0.7710\n",
      "Epoch 2/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.9993 - sparse_categorical_accuracy: 0.8022 - val_loss: 0.8749 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 3/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7969 - sparse_categorical_accuracy: 0.8331 - val_loss: 0.7424 - val_sparse_categorical_accuracy: 0.8418\n",
      "Epoch 4/43\n",
      "54000/54000 [==============================] - 1s 19us/sample - loss: 0.6967 - sparse_categorical_accuracy: 0.8463 - val_loss: 0.6678 - val_sparse_categorical_accuracy: 0.8532\n",
      "Epoch 5/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.6360 - sparse_categorical_accuracy: 0.8551 - val_loss: 0.6194 - val_sparse_categorical_accuracy: 0.8582\n",
      "Epoch 6/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5949 - sparse_categorical_accuracy: 0.8613 - val_loss: 0.5850 - val_sparse_categorical_accuracy: 0.8637\n",
      "Epoch 7/43\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.5648 - sparse_categorical_accuracy: 0.8662 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.8698\n",
      "Epoch 8/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5418 - sparse_categorical_accuracy: 0.8692 - val_loss: 0.5393 - val_sparse_categorical_accuracy: 0.8725\n",
      "Epoch 9/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5234 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.5230 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 10/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5084 - sparse_categorical_accuracy: 0.8753 - val_loss: 0.5095 - val_sparse_categorical_accuracy: 0.8763\n",
      "Epoch 11/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4959 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.4984 - val_sparse_categorical_accuracy: 0.8778\n",
      "Epoch 12/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4853 - sparse_categorical_accuracy: 0.8804 - val_loss: 0.4886 - val_sparse_categorical_accuracy: 0.8802\n",
      "Epoch 13/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4760 - sparse_categorical_accuracy: 0.8820 - val_loss: 0.4802 - val_sparse_categorical_accuracy: 0.8810\n",
      "Epoch 14/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4680 - sparse_categorical_accuracy: 0.8839 - val_loss: 0.4728 - val_sparse_categorical_accuracy: 0.8818\n",
      "Epoch 15/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4609 - sparse_categorical_accuracy: 0.8849 - val_loss: 0.4663 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 16/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4546 - sparse_categorical_accuracy: 0.8867 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.8823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4489 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.4552 - val_sparse_categorical_accuracy: 0.8843\n",
      "Epoch 18/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4438 - sparse_categorical_accuracy: 0.8887 - val_loss: 0.4506 - val_sparse_categorical_accuracy: 0.8855\n",
      "Epoch 19/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4391 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.4462 - val_sparse_categorical_accuracy: 0.8865\n",
      "Epoch 20/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4349 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.4423 - val_sparse_categorical_accuracy: 0.8882\n",
      "Epoch 21/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4309 - sparse_categorical_accuracy: 0.8919 - val_loss: 0.4387 - val_sparse_categorical_accuracy: 0.8890\n",
      "Epoch 22/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4273 - sparse_categorical_accuracy: 0.8926 - val_loss: 0.4355 - val_sparse_categorical_accuracy: 0.8892\n",
      "Epoch 23/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4240 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.4324 - val_sparse_categorical_accuracy: 0.8903\n",
      "Epoch 24/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4209 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.4295 - val_sparse_categorical_accuracy: 0.8907\n",
      "Epoch 25/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4180 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8910\n",
      "Epoch 26/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4153 - sparse_categorical_accuracy: 0.8955 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8913\n",
      "Epoch 27/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4128 - sparse_categorical_accuracy: 0.8962 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8933\n",
      "Epoch 28/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4104 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.4199 - val_sparse_categorical_accuracy: 0.8930\n",
      "Epoch 29/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4082 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.4178 - val_sparse_categorical_accuracy: 0.8932\n",
      "Epoch 30/43\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.4061 - sparse_categorical_accuracy: 0.8977 - val_loss: 0.4159 - val_sparse_categorical_accuracy: 0.8935\n",
      "Epoch 31/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4041 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.4141 - val_sparse_categorical_accuracy: 0.8943\n",
      "Epoch 32/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4022 - sparse_categorical_accuracy: 0.8987 - val_loss: 0.4125 - val_sparse_categorical_accuracy: 0.8945\n",
      "Epoch 33/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4004 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.4109 - val_sparse_categorical_accuracy: 0.8948\n",
      "Epoch 34/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3987 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4094 - val_sparse_categorical_accuracy: 0.8953\n",
      "Epoch 35/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3971 - sparse_categorical_accuracy: 0.9001 - val_loss: 0.4078 - val_sparse_categorical_accuracy: 0.8957\n",
      "Epoch 36/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3955 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8967\n",
      "Epoch 37/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3941 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8962\n",
      "Epoch 38/43\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.3927 - sparse_categorical_accuracy: 0.9010 - val_loss: 0.4039 - val_sparse_categorical_accuracy: 0.8970\n",
      "Epoch 39/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3913 - sparse_categorical_accuracy: 0.9014 - val_loss: 0.4026 - val_sparse_categorical_accuracy: 0.8967\n",
      "Epoch 40/43\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.3900 - sparse_categorical_accuracy: 0.9016 - val_loss: 0.4016 - val_sparse_categorical_accuracy: 0.8977\n",
      "Epoch 41/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3888 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.4004 - val_sparse_categorical_accuracy: 0.8970\n",
      "Epoch 42/43\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.3876 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.3993 - val_sparse_categorical_accuracy: 0.8982\n",
      "Epoch 43/43\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.3865 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.3984 - val_sparse_categorical_accuracy: 0.8980\n",
      "Validation loss: 0.39837003123760223, Validation Accuracy: 89.80000019073486\n",
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/48\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 1.9364 - sparse_categorical_accuracy: 0.4346 - val_loss: 1.6173 - val_sparse_categorical_accuracy: 0.6522\n",
      "Epoch 2/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 1.4203 - sparse_categorical_accuracy: 0.7228 - val_loss: 1.2565 - val_sparse_categorical_accuracy: 0.7588\n",
      "Epoch 3/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 1.1491 - sparse_categorical_accuracy: 0.7827 - val_loss: 1.0568 - val_sparse_categorical_accuracy: 0.7935\n",
      "Epoch 4/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.9905 - sparse_categorical_accuracy: 0.8066 - val_loss: 0.9334 - val_sparse_categorical_accuracy: 0.8125\n",
      "Epoch 5/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.8878 - sparse_categorical_accuracy: 0.8211 - val_loss: 0.8497 - val_sparse_categorical_accuracy: 0.8243\n",
      "Epoch 6/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.8159 - sparse_categorical_accuracy: 0.8311 - val_loss: 0.7894 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 7/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7626 - sparse_categorical_accuracy: 0.8387 - val_loss: 0.7436 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 8/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7213 - sparse_categorical_accuracy: 0.8441 - val_loss: 0.7075 - val_sparse_categorical_accuracy: 0.8442\n",
      "Epoch 9/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6883 - sparse_categorical_accuracy: 0.8479 - val_loss: 0.6783 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 10/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6613 - sparse_categorical_accuracy: 0.8524 - val_loss: 0.6540 - val_sparse_categorical_accuracy: 0.8515\n",
      "Epoch 11/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.6386 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.6335 - val_sparse_categorical_accuracy: 0.8547\n",
      "Epoch 12/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6193 - sparse_categorical_accuracy: 0.8583 - val_loss: 0.6159 - val_sparse_categorical_accuracy: 0.8570\n",
      "Epoch 13/48\n",
      "54000/54000 [==============================] - 1s 19us/sample - loss: 0.6027 - sparse_categorical_accuracy: 0.8608 - val_loss: 0.6007 - val_sparse_categorical_accuracy: 0.8603\n",
      "Epoch 14/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5881 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.5873 - val_sparse_categorical_accuracy: 0.8613\n",
      "Epoch 15/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5753 - sparse_categorical_accuracy: 0.8654 - val_loss: 0.5753 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 16/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5639 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.5648 - val_sparse_categorical_accuracy: 0.8658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5536 - sparse_categorical_accuracy: 0.8686 - val_loss: 0.5552 - val_sparse_categorical_accuracy: 0.8687\n",
      "Epoch 18/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5444 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.5465 - val_sparse_categorical_accuracy: 0.8697\n",
      "Epoch 19/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5359 - sparse_categorical_accuracy: 0.8711 - val_loss: 0.5387 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 20/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5283 - sparse_categorical_accuracy: 0.8726 - val_loss: 0.5316 - val_sparse_categorical_accuracy: 0.8708\n",
      "Epoch 21/48\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.5212 - sparse_categorical_accuracy: 0.8731 - val_loss: 0.5249 - val_sparse_categorical_accuracy: 0.8718\n",
      "Epoch 22/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5147 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.5188 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 23/48\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.5088 - sparse_categorical_accuracy: 0.8758 - val_loss: 0.5131 - val_sparse_categorical_accuracy: 0.8742\n",
      "Epoch 24/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5032 - sparse_categorical_accuracy: 0.8767 - val_loss: 0.5079 - val_sparse_categorical_accuracy: 0.8755\n",
      "Epoch 25/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4980 - sparse_categorical_accuracy: 0.8779 - val_loss: 0.5030 - val_sparse_categorical_accuracy: 0.8757\n",
      "Epoch 26/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4932 - sparse_categorical_accuracy: 0.8789 - val_loss: 0.4984 - val_sparse_categorical_accuracy: 0.8768\n",
      "Epoch 27/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4886 - sparse_categorical_accuracy: 0.8798 - val_loss: 0.4941 - val_sparse_categorical_accuracy: 0.8780\n",
      "Epoch 28/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4844 - sparse_categorical_accuracy: 0.8805 - val_loss: 0.4901 - val_sparse_categorical_accuracy: 0.8777\n",
      "Epoch 29/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4804 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.4863 - val_sparse_categorical_accuracy: 0.8778\n",
      "Epoch 30/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4766 - sparse_categorical_accuracy: 0.8820 - val_loss: 0.4828 - val_sparse_categorical_accuracy: 0.8782\n",
      "Epoch 31/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4730 - sparse_categorical_accuracy: 0.8828 - val_loss: 0.4794 - val_sparse_categorical_accuracy: 0.8793\n",
      "Epoch 32/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4697 - sparse_categorical_accuracy: 0.8834 - val_loss: 0.4762 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 33/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4665 - sparse_categorical_accuracy: 0.8839 - val_loss: 0.4731 - val_sparse_categorical_accuracy: 0.8810\n",
      "Epoch 34/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4634 - sparse_categorical_accuracy: 0.8846 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.8810\n",
      "Epoch 35/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4605 - sparse_categorical_accuracy: 0.8853 - val_loss: 0.4675 - val_sparse_categorical_accuracy: 0.8820\n",
      "Epoch 36/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4577 - sparse_categorical_accuracy: 0.8857 - val_loss: 0.4649 - val_sparse_categorical_accuracy: 0.8827\n",
      "Epoch 37/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4551 - sparse_categorical_accuracy: 0.8861 - val_loss: 0.4624 - val_sparse_categorical_accuracy: 0.8827\n",
      "Epoch 38/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4526 - sparse_categorical_accuracy: 0.8868 - val_loss: 0.4600 - val_sparse_categorical_accuracy: 0.8830\n",
      "Epoch 39/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4502 - sparse_categorical_accuracy: 0.8872 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.8828\n",
      "Epoch 40/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4479 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.4556 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 41/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4457 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8837\n",
      "Epoch 42/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4436 - sparse_categorical_accuracy: 0.8886 - val_loss: 0.4515 - val_sparse_categorical_accuracy: 0.8842\n",
      "Epoch 43/48\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4415 - sparse_categorical_accuracy: 0.8891 - val_loss: 0.4496 - val_sparse_categorical_accuracy: 0.8842\n",
      "Epoch 44/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4395 - sparse_categorical_accuracy: 0.8894 - val_loss: 0.4477 - val_sparse_categorical_accuracy: 0.8845\n",
      "Epoch 45/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4377 - sparse_categorical_accuracy: 0.8896 - val_loss: 0.4459 - val_sparse_categorical_accuracy: 0.8848\n",
      "Epoch 46/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4358 - sparse_categorical_accuracy: 0.8901 - val_loss: 0.4442 - val_sparse_categorical_accuracy: 0.8852\n",
      "Epoch 47/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4341 - sparse_categorical_accuracy: 0.8904 - val_loss: 0.4425 - val_sparse_categorical_accuracy: 0.8858\n",
      "Epoch 48/48\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4324 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.4410 - val_sparse_categorical_accuracy: 0.8857\n",
      "Validation loss: 0.44096091389656067, Validation Accuracy: 88.56666684150696\n",
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 1.8017 - sparse_categorical_accuracy: 0.5104 - val_loss: 1.3984 - val_sparse_categorical_accuracy: 0.7183\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 1.1916 - sparse_categorical_accuracy: 0.7674 - val_loss: 1.0426 - val_sparse_categorical_accuracy: 0.7963\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.9448 - sparse_categorical_accuracy: 0.8104 - val_loss: 0.8747 - val_sparse_categorical_accuracy: 0.8208\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.8162 - sparse_categorical_accuracy: 0.8292 - val_loss: 0.7776 - val_sparse_categorical_accuracy: 0.8357\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.7372 - sparse_categorical_accuracy: 0.8407 - val_loss: 0.7141 - val_sparse_categorical_accuracy: 0.8443\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6832 - sparse_categorical_accuracy: 0.8484 - val_loss: 0.6688 - val_sparse_categorical_accuracy: 0.8510\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.6437 - sparse_categorical_accuracy: 0.8538 - val_loss: 0.6349 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6134 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.6083 - val_sparse_categorical_accuracy: 0.8613\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5893 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5868 - val_sparse_categorical_accuracy: 0.8648\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5696 - sparse_categorical_accuracy: 0.8653 - val_loss: 0.5691 - val_sparse_categorical_accuracy: 0.8675\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5531 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.5541 - val_sparse_categorical_accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5391 - sparse_categorical_accuracy: 0.8695 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8708\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5271 - sparse_categorical_accuracy: 0.8717 - val_loss: 0.5301 - val_sparse_categorical_accuracy: 0.8717\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5165 - sparse_categorical_accuracy: 0.8736 - val_loss: 0.5204 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5072 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.5118 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4989 - sparse_categorical_accuracy: 0.8771 - val_loss: 0.5041 - val_sparse_categorical_accuracy: 0.8767\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.4915 - sparse_categorical_accuracy: 0.8788 - val_loss: 0.4971 - val_sparse_categorical_accuracy: 0.8775\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.4848 - sparse_categorical_accuracy: 0.8802 - val_loss: 0.4908 - val_sparse_categorical_accuracy: 0.8790\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4787 - sparse_categorical_accuracy: 0.8817 - val_loss: 0.4853 - val_sparse_categorical_accuracy: 0.8810\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4732 - sparse_categorical_accuracy: 0.8829 - val_loss: 0.4800 - val_sparse_categorical_accuracy: 0.8822\n",
      "Validation loss: 0.4799809069633484, Validation Accuracy: 88.21666836738586\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs\n",
    "!rm -rf ./logs_linear/model_selection\n",
    "\n",
    "\n",
    "# Hyper-parameters ranges of the model\n",
    "l1_regularization = 0.\n",
    "l2_regularization = (1e-3, 1e-5)\n",
    "learning_rate = (1e-3, 5e-3)\n",
    "batch_size = 64\n",
    "no_epochs = (20, 50)\n",
    "\n",
    "\n",
    "# Data structure to store the best configuration tried on the VALIDATION set\n",
    "best_config = {\"val_score\": 0.,\n",
    "               \"l1_regularization\": None,\n",
    "               \"l2_regularization\": None,\n",
    "               \"learning_rate\": None,\n",
    "               \"batch_size\": None,\n",
    "               \"no_epochs\": None,\n",
    "               \"config_id\": None,\n",
    "               }\n",
    "\n",
    "no_random_searches = 5\n",
    "\n",
    "for i in range(no_random_searches):\n",
    "    \n",
    "    # Select a random configuration\n",
    "    l2 = random.uniform(l2_regularization[0], l2_regularization[1])\n",
    "    lr = random.uniform(learning_rate[0], learning_rate[1])\n",
    "    epochs = random.randint(no_epochs[0], no_epochs[1])\n",
    "    \n",
    "    # We will define a linear model with the Sequential API\n",
    "    model = Sequential()\n",
    "\n",
    "    # Let's add the connection between input and output\n",
    "    model.add(Dense(units=dim_target,  # depends on the task at hand\n",
    "                     activation='linear',  # the identiy mapping (no need to use non-linearities here, it's a linear model!)\n",
    "                     use_bias=True,  # we want to implement Wx + b where b is the bias\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1_regularization, l2=l2)  # Regularizes the weight matrix W                 \n",
    "                     ))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.SGD(lr=lr),  # Optimizer\n",
    "                   # Loss function to minimize\n",
    "                   loss=losses.SparseCategoricalCrossentropy(from_logits=True),  # Sparse will convert categorical labels for us in one-hot form!\n",
    "                   # List of metrics to monitor\n",
    "                   metrics=[metrics.sparse_categorical_accuracy])\n",
    "\n",
    "\n",
    "    # Set up a log folder in which we will store the output to be displayed on TensorBoard\n",
    "    logdir = f\"logs_linear/model_selection/config_{i}/fit/\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "    # Train the Model!\n",
    "    # Note: fit has also the chance to specify a validation split percentage\n",
    "    print('# Fit model on training data')\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        # We pass some validation for\n",
    "                        # monitoring validation loss and metrics\n",
    "                        # at the end of each epoch\n",
    "                        validation_data=(x_valid, y_valid),\n",
    "                        callbacks=[tensorboard_callback]\n",
    "                        )\n",
    "    \n",
    "    score = model.evaluate(x_valid, y_valid, batch_size=128, verbose=0)  # Verbose 0 does not show any log\n",
    "    print(f'Validation loss: {score[0]}, Validation Accuracy: {score[1]*100}')\n",
    "    \n",
    "    if score[1]*100 >= best_config['val_score']:\n",
    "        best_config['val_score'] = score[1]*100\n",
    "        best_config['l1_regularization'] = l1_regularization\n",
    "        best_config['l2_regularization'] = l2\n",
    "        best_config['learning_rate'] = lr\n",
    "        best_config['no_epochs'] = no_epochs\n",
    "        best_config['batch_size'] = batch_size\n",
    "        best_config['config_id'] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_score': 90.01666903495789, 'l1_regularization': 0.0, 'l2_regularization': 0.0005833206654935294, 'learning_rate': 0.0032105085823332024, 'batch_size': 64, 'no_epochs': (20, 50), 'config_id': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool! We automatically found our best candidate :) now test it on test! Then there's no turning back ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91e15e4e1490bda5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91e15e4e1490bda5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6015;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize your winning model\n",
    "%tensorboard --logdir ./logs_linear/model_selection/config_{best_config[\"config_id\"]}/fit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4486263397216797, Test Accuracy: 89.06000256538391\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)  # Verbose 0 does not show any log\n",
    "print(f'Test loss: {score[0]}, Test Accuracy: {score[1]*100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
