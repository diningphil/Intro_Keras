{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 101 - Autoencoders for Anomaly Detection & Image Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will program simple Keras examples using the **Functional** and Sequential API.\n",
    "\n",
    "### This will be an instance of unsupervised learning --> we do not need target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# For variational auto-encoder\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt # For plotting purposes\n",
    "\n",
    "\n",
    "# Bug still not solved, resort to old keras module (see below)\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Use the above import when the bug is solved\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "# ------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension (for visualization purposes)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each MNIST image has shape (28,28)\n",
    "mnist_img_rows, mnist_img_cols = 28, 28\n",
    "dim_target = 10\n",
    "\n",
    "# Load the MNIST dataset, which has already been splitted into training and test set\n",
    "(mnist_train, mnist_y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the shape of our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 examples of (28, 28) features each value\n",
      "Training targets have shape (60000,) i.e. we will predict a constant target value for each example\n",
      "\n",
      "Test set has 10000 examples of (28, 28) features each value\n",
      "Test targets have shape (10000,) i.e. we will predict a constant target value for each example\n",
      "\n",
      "Examples have type uint8, targets have type uint8\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set has {mnist_train.shape[0]} examples of {mnist_train.shape[1:]} features each value')\n",
    "print(f'Training targets have shape {mnist_y_train.shape} i.e. we will predict a constant target value for each example')\n",
    "print('')\n",
    "print(f'Test set has {x_test.shape[0]} examples of {x_test.shape[1:]} features each value')\n",
    "print(f'Test targets have shape {y_test.shape} i.e. we will predict a constant target value for each example')\n",
    "print('')\n",
    "print(f'Examples have type {mnist_train.dtype}, targets have type {mnist_y_train.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a validation set from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subset(x_dataset, y_dataset, subset_percentage=10):\n",
    "    # Choose a seed to obtain the same split\n",
    "    random.seed(42)\n",
    "    \n",
    "    no_samples = x_dataset.shape[0]\n",
    "\n",
    "    # Generate a list of indices\n",
    "    indices = [i for i in range(no_samples)]\n",
    "\n",
    "    # We want the x% of the training set to be used as a validation set\n",
    "    no_subset_samples = int((no_samples/100)*subset_percentage)\n",
    "\n",
    "    # In place shuffling of the data structure\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Display the indices\n",
    "    # print(f'Validation indices are: {indices[:no_subset_samples]}')\n",
    "\n",
    "    # The subset will correspond to the first \"no_subset_samples\" shuffled indices\n",
    "    x_subset = x_dataset[indices[:no_subset_samples], :]\n",
    "    y_subset = y_dataset[indices[:no_subset_samples]]\n",
    "\n",
    "    # The rest of the data will correspond to other shuffled indices\n",
    "    new_x_dataset = x_dataset[indices[no_subset_samples:], :]\n",
    "    new_y_dataset = y_dataset[indices[no_subset_samples:]]\n",
    "    \n",
    "    return (new_x_dataset, new_y_dataset), (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a validation set from the TRAINING set.\n",
    "# You can also use this function to split you entire dataset into training, validation and test sets when these are not given.\n",
    "(x_train, y_train), (x_valid, y_valid) = extract_subset(mnist_train, mnist_y_train, subset_percentage=10)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], -1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First define the hyper-parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of the model\n",
    "l1_regularization = 0.  # L1 regularization to be applied to the weight matrix W\n",
    "l2_regularization = 1e-5  # L2 regularization to be applied to the weight matrix W\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "batch_size = 64  # Mini-batch training for stochastic gradient descent\n",
    "no_epochs = 20  # Maximum number of epochs to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define a Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://i0.wp.com/mlexplained.com/wp-content/uploads/2017/12/VAE_complete.png?resize=1024%2C439&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image taken from [this blog post](https://mlexplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/). \n",
    "\n",
    "### Now, let's define a function that provides the epsilon of the picture and multiplies it with Sigma (the variance associated with the latent representation). We are not interested in the math behind it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick (no need to know the details)\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]  # get dimension of mini-batch\n",
    "    dim = K.int_shape(z_mean)[1]  # get dimension of each z\n",
    "\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a linear model with the Sequential API\n",
    "linear = Sequential()\n",
    "\n",
    "# Let's add the connection between input and output\n",
    "linear.add(Dense(units=dim_target,  # depends on the task at hand\n",
    "                 activation='linear',  # the identiy mapping (no need to use non-linearities here, it's a linear model!)\n",
    "                 use_bias=True,  # we want to implement Wx + b where b is the bias\n",
    "                 kernel_regularizer=regularizers.l1_l2(l1=l1_regularization, l2=l2_regularization)  # Regularizes the weight matrix W                 \n",
    "                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.compile(optimizer=optimizers.SGD(lr=learning_rate),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),  # Sparse will convert categorical labels for us in one-hot form!\n",
    "              # List of metrics to monitor\n",
    "              metrics=[metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "51520/54000 [===========================>..] - ETA: 0s - loss: 1.9964 - sparse_categorical_accuracy: 0.3830\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.64533, saving model to checkpoints/linear/cp.ckpt\n",
      "WARNING:tensorflow:From /home/errica/anaconda3/envs/intro-keras/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 1.9814 - sparse_categorical_accuracy: 0.3947 - val_loss: 1.6580 - val_sparse_categorical_accuracy: 0.6453\n",
      "Epoch 2/20\n",
      "51904/54000 [===========================>..] - ETA: 0s - loss: 1.4601 - sparse_categorical_accuracy: 0.7105\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.64533 to 0.75067, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 1.4537 - sparse_categorical_accuracy: 0.7124 - val_loss: 1.2916 - val_sparse_categorical_accuracy: 0.7507\n",
      "Epoch 3/20\n",
      "52928/54000 [============================>.] - ETA: 0s - loss: 1.1791 - sparse_categorical_accuracy: 0.7712\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.75067 to 0.78217, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 1.1767 - sparse_categorical_accuracy: 0.7717 - val_loss: 1.0861 - val_sparse_categorical_accuracy: 0.7822\n",
      "Epoch 4/20\n",
      "53120/54000 [============================>.] - ETA: 0s - loss: 1.0143 - sparse_categorical_accuracy: 0.7975\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.78217 to 0.80233, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 1.0132 - sparse_categorical_accuracy: 0.7977 - val_loss: 0.9574 - val_sparse_categorical_accuracy: 0.8023\n",
      "Epoch 5/20\n",
      "52992/54000 [============================>.] - ETA: 0s - loss: 0.9072 - sparse_categorical_accuracy: 0.8127\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.80233 to 0.81400, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.9063 - sparse_categorical_accuracy: 0.8127 - val_loss: 0.8694 - val_sparse_categorical_accuracy: 0.8140\n",
      "Epoch 6/20\n",
      "51136/54000 [===========================>..] - ETA: 0s - loss: 0.8315 - sparse_categorical_accuracy: 0.8235\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.81400 to 0.82233, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.8308 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.8055 - val_sparse_categorical_accuracy: 0.8223\n",
      "Epoch 7/20\n",
      "51648/54000 [===========================>..] - ETA: 0s - loss: 0.7754 - sparse_categorical_accuracy: 0.8317\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.82233 to 0.83067, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7746 - sparse_categorical_accuracy: 0.8316 - val_loss: 0.7566 - val_sparse_categorical_accuracy: 0.8307\n",
      "Epoch 8/20\n",
      "52480/54000 [============================>.] - ETA: 0s - loss: 0.7316 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.83067 to 0.83717, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.7308 - sparse_categorical_accuracy: 0.8377 - val_loss: 0.7179 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 9/20\n",
      "52224/54000 [============================>.] - ETA: 0s - loss: 0.6958 - sparse_categorical_accuracy: 0.8420\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.83717 to 0.84250, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.6956 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.6863 - val_sparse_categorical_accuracy: 0.8425\n",
      "Epoch 10/20\n",
      "53120/54000 [============================>.] - ETA: 0s - loss: 0.6663 - sparse_categorical_accuracy: 0.8463\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.84250 to 0.84783, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.6666 - sparse_categorical_accuracy: 0.8461 - val_loss: 0.6601 - val_sparse_categorical_accuracy: 0.8478\n",
      "Epoch 11/20\n",
      "53824/54000 [============================>.] - ETA: 0s - loss: 0.6424 - sparse_categorical_accuracy: 0.8499\n",
      "Epoch 00011: val_sparse_categorical_accuracy improved from 0.84783 to 0.84900, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.6423 - sparse_categorical_accuracy: 0.8499 - val_loss: 0.6379 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 12/20\n",
      "52736/54000 [============================>.] - ETA: 0s - loss: 0.6212 - sparse_categorical_accuracy: 0.8530\n",
      "Epoch 00012: val_sparse_categorical_accuracy improved from 0.84900 to 0.85150, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.6215 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.6187 - val_sparse_categorical_accuracy: 0.8515\n",
      "Epoch 13/20\n",
      "53696/54000 [============================>.] - ETA: 0s - loss: 0.6039 - sparse_categorical_accuracy: 0.8557\n",
      "Epoch 00013: val_sparse_categorical_accuracy improved from 0.85150 to 0.85383, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.6035 - sparse_categorical_accuracy: 0.8559 - val_loss: 0.6020 - val_sparse_categorical_accuracy: 0.8538\n",
      "Epoch 14/20\n",
      "53888/54000 [============================>.] - ETA: 0s - loss: 0.5878 - sparse_categorical_accuracy: 0.8579\n",
      "Epoch 00014: val_sparse_categorical_accuracy improved from 0.85383 to 0.85617, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5878 - sparse_categorical_accuracy: 0.8578 - val_loss: 0.5873 - val_sparse_categorical_accuracy: 0.8562\n",
      "Epoch 15/20\n",
      "51392/54000 [===========================>..] - ETA: 0s - loss: 0.5742 - sparse_categorical_accuracy: 0.8598\n",
      "Epoch 00015: val_sparse_categorical_accuracy improved from 0.85617 to 0.85717, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.5738 - sparse_categorical_accuracy: 0.8600 - val_loss: 0.5743 - val_sparse_categorical_accuracy: 0.8572\n",
      "Epoch 16/20\n",
      "51392/54000 [===========================>..] - ETA: 0s - loss: 0.5624 - sparse_categorical_accuracy: 0.8617\n",
      "Epoch 00016: val_sparse_categorical_accuracy improved from 0.85717 to 0.85917, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5614 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5626 - val_sparse_categorical_accuracy: 0.8592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "53952/54000 [============================>.] - ETA: 0s - loss: 0.5502 - sparse_categorical_accuracy: 0.8637\n",
      "Epoch 00017: val_sparse_categorical_accuracy improved from 0.85917 to 0.86167, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5502 - sparse_categorical_accuracy: 0.8637 - val_loss: 0.5521 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 18/20\n",
      "52736/54000 [============================>.] - ETA: 0s - loss: 0.5402 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 00018: val_sparse_categorical_accuracy improved from 0.86167 to 0.86350, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.5401 - sparse_categorical_accuracy: 0.8654 - val_loss: 0.5426 - val_sparse_categorical_accuracy: 0.8635\n",
      "Epoch 19/20\n",
      "51648/54000 [===========================>..] - ETA: 0s - loss: 0.5301 - sparse_categorical_accuracy: 0.8674\n",
      "Epoch 00019: val_sparse_categorical_accuracy improved from 0.86350 to 0.86567, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.5309 - sparse_categorical_accuracy: 0.8667 - val_loss: 0.5338 - val_sparse_categorical_accuracy: 0.8657\n",
      "Epoch 20/20\n",
      "53696/54000 [============================>.] - ETA: 0s - loss: 0.5221 - sparse_categorical_accuracy: 0.8685\n",
      "Epoch 00020: val_sparse_categorical_accuracy improved from 0.86567 to 0.86667, saving model to checkpoints/linear/cp.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/linear/cp.ckpt/assets\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.5225 - sparse_categorical_accuracy: 0.8683 - val_loss: 0.5259 - val_sparse_categorical_accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_linear\n",
    "\n",
    "# Set up a log folder in which we will store the output to be displayed on TensorBoard\n",
    "logdir = \"logs_linear/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Chekpoint path for storing our model\n",
    "checkpoint_path = \"checkpoints/linear/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# There are many possible ways to store a model, for example every n epochs. Check the documentation, very useful!\n",
    "save_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,  # Save only the best epoch according to valid set\n",
    "                                                 monitor=\"val_sparse_categorical_accuracy\",  # Save according to valid acc\n",
    "                                                 verbose=1,\n",
    "                                                 save_weights_only=False,  # we want to save the graph as well  \n",
    "                                                 )  \n",
    "\n",
    "# Train the Model!\n",
    "# Note: fit has also the chance to specify a validation split percentage\n",
    "print('# Fit model on training data')\n",
    "history = linear.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=no_epochs,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[save_callback, tensorboard_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6db877f210820a86\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6db877f210820a86\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize your model\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model from disk (for demo purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time! \n",
    "### We cannot change the hyper-parameters once we evaluate on test!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.493513893032074, Test Accuracy: 87.92999982833862\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)  # Verbose 0 does not show any log\n",
    "print(f'Test loss: {score[0]}, Test Accuracy: {score[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## We found the TEST accuracy of our linear model. There's no turning back now, you should go to your boss to report this result.\n",
    "## If you start again changing hyper-parameters and see if it improves on TEST, that is dead wrong! You are using the TEST as a VALIDATION !!\n",
    "\n",
    "### Instead, take some more time to validate a broader range of configurations on the VALIDATION test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's classify one example from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8a8fc2210>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape(mnist_img_rows, mnist_img_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class that has maximum probability associated to the input is: 7\n"
     ]
    }
   ],
   "source": [
    "# Add a softmax activation to get probabilities\n",
    "model.add(Softmax())\n",
    "\n",
    "# Predict output\n",
    "y = model.predict(np.expand_dims(x_test[0,:], 0))\n",
    "\n",
    "print(f'The class that has maximum probability associated to the input is: {np.argmax(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise time! Change the above code to train an MLP!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
