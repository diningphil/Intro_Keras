{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:100px\">\n",
    "\n",
    "<div style=\"display:inline-block; width:77%; vertical-align:middle;\">\n",
    "    <div>\n",
    "        <b>Author</b>: <a href=\"http://pages.di.unipi.it/castellana/\">Daniele Castellana</a>\n",
    "    </div>\n",
    "    <div>\n",
    "        PhD student at the Univeristy of Pisa and member of the Computational Intelligence & Machine Learning Group (<a href=\"http://www.di.unipi.it/groups/ciml/\">CIML</a>)\n",
    "    </div>\n",
    "    <div>\n",
    "        <b>Mail</b>: <a href=\"mailto:daniele.castellana@di.unipi.it\">daniele.castellana@di.unipi.it</a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display:inline-block; width: 10%; vertical-align:middle;\">\n",
    "    <img align=\"right\" width=\"100%\" src=\"https://upload.wikimedia.org/wikipedia/it/7/72/Stemma_unipi.png\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display:inline-block; width: 10%; vertical-align:middle;\">\n",
    "    <img align=\"right\" width=\"100%\" src=\"http://www.di.unipi.it/groups/ciml/Home_files/loghi/logo_ciml-restyling2018.svg\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Sentiment Classification\n",
    "\n",
    "A typical NLP task is the sentiment classification. The task requires to assingn a sentiment to natural language sentences.\n",
    "\n",
    "The problem that we will use to demonstrate sequence classification is the IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified.\n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
    "\n",
    "Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. \n",
    "\n",
    "Howeverm, when we work on natural language data, we should pay a lot of effort on the data pre-processing since most of the data are not clean.\n",
    "\n",
    "Hence, we will start from the raw data ([download](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch09/movie_data.csv.gz)).\n",
    "\n",
    "Whe everything is clear, **try to improve the model performance using what you have larnt during this course!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/IMDB_movie_data.csv', encoding='utf-8')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input shape is (25000,)\n",
      "The output shape is (25000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = df.loc[:25000-1, 'review'].values\n",
    "y_train = df.loc[:25000-1, 'sentiment'].values\n",
    "\n",
    "x_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "print('The input shape is {}\\n'\n",
    "      'The output shape is {}'.format(x_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "\n",
    "As we can see from the example above, theere are a lot of undesiribles token in the text: for example, the html tags.\n",
    "\n",
    "We use regular expression to remove the br tag along with punctuations and brackets. The input text should become a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to clean the data\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def clean_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = clean_reviews(x_train)\n",
    "x_test = clean_reviews(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in oj simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70s they discover the criminal and a net of power and money to cover the murder murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is seven title brazil not available'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the word vocabulary\n",
    "\n",
    "The LSTM are not able to understand the words. Hence, we need to map each word in an integer number which represnet the index of that words in a vocabulary.\n",
    "\n",
    "The vocabular construction and the translation from seq-of-words to seq-of-int can be done easily using the Keras Tokenizer ([doc page](https://keras.io/preprocessing/text/)).\n",
    "\n",
    "In order to speed-up the training, we get only the 10000 most freqeuent words. This option is enabled setting the parameter **num_words** to the tokenizer constructor. Moreover, we specify the special token OOV to represent all the words that are not in the vocabualary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# we use only the 10k most frequent words\n",
    "num_words = 10000\n",
    "tokenizer_obj = Tokenizer(num_words=num_words, oov_token='OOV')\n",
    "# the tokenizer will not use the idx 0 because it will be used to pad sequences\n",
    "all_reviews = x_train + x_test\n",
    "tokenizer_obj.fit_on_texts(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8, 5774,    2, 2191, 4079,    1, 4942, 1608, 1114,    6,    2,\n",
       "        297,  707, 1597,    5, 7024,    1,    1, 8719,   20,    2,    1,\n",
       "        311, 3611,    5, 2118,   59,   13, 1909,    8,    2, 8720,    5,\n",
       "         41,  320,    3,   41,  590, 4640,    1, 1699,  104,  149,  305,\n",
       "          2,  569,  966,    1, 1467,    1,   37,    7,    4, 1105,  982,\n",
       "       1354,   12,   45, 3019,    8, 5880,   15,    1,    8,    1, 5748,\n",
       "       3161,    3, 1634,    6,    1, 1089,    6, 3882,    2,  420,   16,\n",
       "         24, 1959, 1791, 2237, 3803, 3505,   16,    2, 1264,    5,  488,\n",
       "          4,  274,    2, 5380,    1,    3,   77,   21, 2550,   92,   18,\n",
       "         16,    2, 1438,    5,    2, 4993, 1354, 1337, 9770,  614,    1,\n",
       "         12,   13,    8, 2760,    5,    2, 3465,    8,    2,  977,   34,\n",
       "       1837,    2, 1777,    3,    4, 5515,    5,  657,    3,  290,    6,\n",
       "        992,    2,  590,  590,    8,    1,    7,    4,   49,  229,   17,\n",
       "         16,    2,  286,   64,    5,    4,  590,    5,    4, 3482,  149,\n",
       "        154,  249,   12,   13, 2330,   32,    4, 3026, 2191,  645,  419,\n",
       "         13,    4, 3270,    2,  957,    3,  983,  236,  326,   65, 2384,\n",
       "          6,  992,    2,  590,   15,   50,   71, 1699,  149,  192,    4,\n",
       "          1, 1354,    3, 6153,    1,    8, 5880,   13,  478,    6,    1,\n",
       "         86,    2, 4419,  811,   13, 2330,    2,  928,  263,    2, 3465,\n",
       "          5,  966,    3,    2,  237,  479,    5, 4079,    8, 4190,   18,\n",
       "         47,    7,    4,  574,    5,    2, 1413,    8,    2,    1,   56,\n",
       "       2046,    7, 1571,  416, 3535,   21, 1288])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq = tokenizer_obj.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer_obj.texts_to_sequences(x_test)\n",
    "\n",
    "#the text becomes an integer sequence!\n",
    "np.array(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in 1974 the teenager martha OOV maggie grace moves to the high class area of belle OOV OOV connecticut on the OOV night eve of halloween she was murdered in the backyard of her house and her murder remained OOV twenty two years later the writer mark OOV christopher OOV who is a former la detective that has fallen in disgrace for OOV in OOV simpson trial and moved to OOV decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals OOV and do not welcome them but with the support of the retired detective steve carroll robert OOV that was in charge of the investigation in the 70s they discover the criminal and a net of power and money to cover the murder murder in OOV is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a OOV detective and convicted OOV in disgrace was able to OOV how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the OOV my vote is seven title brazil not available']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can revert the process\n",
    "tokenizer_obj.sequences_to_texts(x_train_seq[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence alignment\n",
    "\n",
    "The last step of the data preparation is the sequence alignment since the Keras LSTM works on sequence with the same length.\n",
    "\n",
    "Again, we can use the **pad_sequences()** Keras function to pad the short sequence and to trim the long one. The value used for the padding is the 0, which is already excluded by the tokenizer during the vocabulary construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training shape is (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# each sequence have a different lenght\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# finde the max len\n",
    "# maxlen = max([len(l) for l in x_train_seq+x_test_seq]) = 2473 is too large\n",
    "# we fix the maxlen to 500\n",
    "maxlen = 500\n",
    "# the value used for padding is 0\n",
    "x_train_seq = pad_sequences(x_train_seq, maxlen=maxlen)\n",
    "x_test_seq = pad_sequences(x_test_seq, maxlen=maxlen)\n",
    "\n",
    "\n",
    "print('The training shape is {}'.format(x_train_seq.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the embedding layer\n",
    "\n",
    "We decide to use an embedding layer to map the word id in a vector with 100 feature.\n",
    "\n",
    "The embedding layer can be easily defined using the Keras **EmbeddingLayer**; during its initialisation we need to specify the number of words and the size of the embedding ([doc page](https://keras.io/layers/embeddings/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # the 0 is the padding\n",
    "    model.add(Embedding(num_words, embedding_dim, mask_zero=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniele Castellana\\Miniconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,117,377\n",
      "Trainable params: 1,117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Daniele Castellana\\Miniconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 31s 6ms/step - loss: 0.6879 - acc: 0.5434 - val_loss: 0.6607 - val_acc: 0.5780\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.6258 - acc: 0.6538 - val_loss: 0.6219 - val_acc: 0.6520\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.4599 - acc: 0.8078 - val_loss: 0.4114 - val_acc: 0.8230\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.2495 - acc: 0.8976 - val_loss: 0.3935 - val_acc: 0.8180\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 35s 7ms/step - loss: 0.1535 - acc: 0.9490 - val_loss: 0.3945 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 34s 7ms/step - loss: 0.0874 - acc: 0.9786 - val_loss: 0.4064 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 32s 6ms/step - loss: 0.0516 - acc: 0.9868 - val_loss: 0.4698 - val_acc: 0.8490\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.0345 - acc: 0.9924 - val_loss: 0.5419 - val_acc: 0.8400\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 49s 10ms/step - loss: 0.0212 - acc: 0.9960 - val_loss: 0.5788 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 45s 9ms/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.6837 - val_acc: 0.8340\n"
     ]
    }
   ],
   "source": [
    "LSTM_class = build_model()\n",
    "print(LSTM_class.summary())\n",
    "\n",
    "# we use less data to speed-up the computation\n",
    "LSTM_class.fit(x_train_seq[:5000,:], y_train[:5000], validation_data=(x_test_seq[:1000,:], y_test[:1000]), epochs=10, batch_size=256, verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the embedding matrix\n",
    "emb_matrix = LSTM_class.layers[0].get_weights()[0]\n",
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar embeddings to the words 'horrible' and 'fantastic'\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "horrible_emb = emb_matrix[tokenizer_obj.word_index['horrible'],:]\n",
    "fantastic_emb = emb_matrix[tokenizer_obj.word_index['fantastic'],:]\n",
    "\n",
    "emb_sim = cosine_similarity(emb_matrix, np.stack((horrible_emb, fantastic_emb), axis=0))\n",
    "\n",
    "emb_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horrible', '4', 'unclear', 'boring', 'avoid']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 5 most similar word to 'horrible'\n",
    "[tokenizer_obj.index_word[i] for i in np.flip(np.argsort(emb_sim[:,0])[-5:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fantastic', 'heights', 'images', 'wonderful', 'ages']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 5 most similar word to 'fantastic'\n",
    "[tokenizer_obj.index_word[i] for i in np.flip(np.argsort(emb_sim[:,1])[-5:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use a pretrained embedding in Keras?\n",
    "\n",
    "There are a lot of word-embeddings that are already pretrained on huge text corpora. In some cases, using the pretrained word-embeddings can boost our model performance.\n",
    "\n",
    "## Glove embeddings\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "More information and download on the [project page](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word embeddings.\n"
     ]
    }
   ],
   "source": [
    "# first of all, we read the .txt files whihc contains the embeddings and we store its content in vocaburaly word_to_embedding.\n",
    "import os\n",
    "word2embedding = {}\n",
    "# we keep embedding_dim = 100\n",
    "with open(os.path.join('data/glove_embs/glove.6B.100d.txt'),'r',encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word2embedding[word] = coefs\n",
    "\n",
    "print('Found {} word embeddings.'.format(len(word2embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding matrix has shape (10000, 100).\n",
      "45 embeddings not found.\n"
     ]
    }
   ],
   "source": [
    "# retrieve the idx2word vocabulary from the tokenizer\n",
    "idx2word = tokenizer_obj.index_word\n",
    "\n",
    "glove_emb_matrix = np.random.randn(num_words, embedding_dim)\n",
    "\n",
    "not_found = 0\n",
    "#we start from 1 to skip the 0 padding\n",
    "for i in range(1,num_words):\n",
    "    # retrieve the word\n",
    "    w = idx2word[i]\n",
    "    \n",
    "    #check if exisists a pretrained embedding\n",
    "    if w in word2embedding:\n",
    "        glove_emb_matrix[i,:] = word2embedding[w]\n",
    "    else:\n",
    "        not_found = not_found+1\n",
    "\n",
    "#remove t\n",
    "print('The embedding matrix has shape {}.\\n'\n",
    "      '{} embeddings not found.'.format(glove_emb_matrix.shape, not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def build_model_with_glove_embs():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(num_words, embedding_dim, mask_zero=True,\n",
    "                        embeddings_initializer=Constant(glove_emb_matrix), trainable=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,117,377\n",
      "Trainable params: 1,117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.6802 - acc: 0.5658 - val_loss: 0.6496 - val_acc: 0.6080\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 45s 9ms/step - loss: 0.6125 - acc: 0.6780 - val_loss: 0.6149 - val_acc: 0.6870\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 49s 10ms/step - loss: 0.5799 - acc: 0.7074 - val_loss: 0.5424 - val_acc: 0.7310\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.4761 - acc: 0.7746 - val_loss: 0.5116 - val_acc: 0.7570\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 44s 9ms/step - loss: 0.3992 - acc: 0.8226 - val_loss: 0.5273 - val_acc: 0.7260\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 48s 10ms/step - loss: 0.3597 - acc: 0.8448 - val_loss: 0.4236 - val_acc: 0.8130\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 41s 8ms/step - loss: 0.3519 - acc: 0.8466 - val_loss: 0.3960 - val_acc: 0.8180\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 49s 10ms/step - loss: 0.2640 - acc: 0.8966 - val_loss: 0.4043 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.3530 - acc: 0.8560 - val_loss: 0.5780 - val_acc: 0.6890\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.4636 - acc: 0.7920 - val_loss: 0.4893 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "glove_LSTM_class = build_model_with_glove_embs()\n",
    "print(glove_LSTM_class.summary())\n",
    "\n",
    "# we use less data to speed-up the computation\n",
    "glove_LSTM_class.fit(x_train_seq[:5000,:], y_train[:5000], validation_data=(x_test_seq[:1000,:], y_test[:1000]), epochs=10, batch_size=256, verbose=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
