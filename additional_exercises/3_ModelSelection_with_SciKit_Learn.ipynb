{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height:100px\">\n",
    "\n",
    "<div style=\"display:inline-block; width:77%; vertical-align:middle;\">\n",
    "    <div>\n",
    "        <b>Author</b>: <a href=\"http://pages.di.unipi.it/castellana/\">Daniele Castellana</a>\n",
    "    </div>\n",
    "    <div>\n",
    "        PhD student at the Univeristy of Pisa and member of the Computational Intelligence & Machine Learning Group (<a href=\"http://www.di.unipi.it/groups/ciml/\">CIML</a>)\n",
    "    </div>\n",
    "    <div>\n",
    "        <b>Mail</b>: <a href=\"mailto:daniele.castellana@di.unipi.it\">daniele.castellana@di.unipi.it</a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display:inline-block; width: 10%; vertical-align:middle;\">\n",
    "    <img align=\"right\" width=\"100%\" src=\"https://upload.wikimedia.org/wikipedia/it/7/72/Stemma_unipi.png\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display:inline-block; width: 10%; vertical-align:middle;\">\n",
    "    <img align=\"right\" width=\"100%\" src=\"http://www.di.unipi.it/groups/ciml/Home_files/loghi/logo_ciml-restyling2018.svg\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "Model selection should be used to determine the appropriate values of the hyper-parameters of the model by using the optimal validation error.\n",
    "\n",
    "## Dataset splitting\n",
    "The dataset shold be split into three sets as follow:\n",
    "- **Training set**: use to update the weights; patterns in this set are repeatedly in random order and the weight update equation are applied after a certain number of patterns\n",
    "- **Validation set**: use to decide when to stop training only by monitoring the error and to select the best model configuration\n",
    "- **Test set**: use to test the performance of the neural network. It should not be used as part of the neural network development and model selection cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set is matrix of size (404, 13).\n",
      "404 is the number of samples and 13 is the number of feature.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# rescale values\n",
    "x_train = StandardScaler().fit(x_train).transform(x_train)\n",
    "x_test = StandardScaler().fit(x_test).transform(x_test)\n",
    "\n",
    "print(\"The training set is matrix of size {}.\\n\"\n",
    "      \"{} is the number of samples and {} is the number of feature.\".format(x_train.shape, x_train.shape[0], x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function which takes the model hyper-parameters as input and return a compiled keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def build_model(n_layers=2, h_dim=64, activation='relu', optimizer='adagrad'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "\n",
    "    n_feature = x_train.shape[1]\n",
    "    \n",
    "    model.add(Dense(h_dim, activation=activation, input_shape=(n_feature,)))\n",
    "    for i in range(n_layers-1):\n",
    "        model.add(Dense(h_dim, activation=activation))\n",
    "    #lienar activation\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation using Scikit-learn\n",
    "We use the library scikit-learn to perform the validation. Keras model can be passed to scikit-learn library using a wrapper which can be found in keras.wrappers.scikit_learn (see [doc page](https://keras.io/scikit-learn-api/)). Also, [this page](https://scikit-learn.org/stable/modules/grid_search.html) contains an user guide for model selection using scikit-learn.\n",
    "\n",
    "## Grid definition\n",
    "The first step is to define the grid of hyper-parameters which should be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the grid\n",
    "n_layers = [1, 2, 3]\n",
    "h_dim = [32, 64, 128]\n",
    "activation = ['relu', 'tanh']\n",
    "optimizer = ['adagrad', 'adam']\n",
    "param_grid = dict(optimizer=optimizer, n_layers=n_layers, h_dim=h_dim, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross-Validation\n",
    "The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.\n",
    "\n",
    "The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained (see [doc page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = KerasRegressor(build_fn=build_model)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "    grid_result = grid.fit(x_train, y_train, epochs=100, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The grid search is too expansive** since we must compute 3 * 3 * 2 * 2 = 36 configurations. Each of them is validated trough a 3-fold validation, obtaining 36 * 3 = 108 fit.\n",
    "\n",
    "## Random Search Cross-Validation\n",
    "When the search space is too big and an exhaustive grid search is too expansive, we can perform a random search over the hyper-parameters grid.\n",
    "\n",
    "RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. \n",
    "\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the **n_iter** parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified (see [doc page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "134/134 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "134/134 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "134/134 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 1s 4ms/step\n",
      "135/135 [==============================] - 1s 4ms/step\n",
      "134/134 [==============================] - 0s 4ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "135/135 [==============================] - 0s 2ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "135/135 [==============================] - 1s 4ms/step\n",
      "135/135 [==============================] - 1s 4ms/step\n",
      "134/134 [==============================] - 1s 4ms/step\n",
      "135/135 [==============================] - 1s 5ms/step\n",
      "135/135 [==============================] - 1s 5ms/step\n",
      "134/134 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniele Castellana\\Miniconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#we can use a random serach\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = KerasRegressor(build_fn=build_model)\n",
    "# scikit learn will randomly choose 10 configurarion over the 36 available\n",
    "randSearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3)\n",
    "rand_result = randSearch.fit(x_train, y_train, epochs=100, batch_size=10, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the results of the random serch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 11.013070 using {'optimizer': 'adam', 'n_layers': 3, 'h_dim': 128, 'activation': 'relu'}\n",
      "16.403987 (5.876551) with: {'optimizer': 'rmsprop', 'n_layers': 2, 'h_dim': 128, 'activation': 'tanh'}\n",
      "11.776609 (2.446961) with: {'optimizer': 'rmsprop', 'n_layers': 1, 'h_dim': 128, 'activation': 'relu'}\n",
      "16.614111 (4.449182) with: {'optimizer': 'adam', 'n_layers': 2, 'h_dim': 128, 'activation': 'tanh'}\n",
      "19.048151 (3.786665) with: {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 128, 'activation': 'tanh'}\n",
      "12.622468 (2.156957) with: {'optimizer': 'rmsprop', 'n_layers': 2, 'h_dim': 128, 'activation': 'relu'}\n",
      "12.403694 (2.396414) with: {'optimizer': 'rmsprop', 'n_layers': 2, 'h_dim': 32, 'activation': 'relu'}\n",
      "15.940225 (5.744311) with: {'optimizer': 'rmsprop', 'n_layers': 3, 'h_dim': 32, 'activation': 'tanh'}\n",
      "15.532436 (3.004790) with: {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 32, 'activation': 'relu'}\n",
      "11.510838 (1.913685) with: {'optimizer': 'adam', 'n_layers': 2, 'h_dim': 128, 'activation': 'relu'}\n",
      "11.013070 (2.739830) with: {'optimizer': 'adam', 'n_layers': 3, 'h_dim': 128, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (-rand_result.best_score_, rand_result.best_params_))\n",
    "means = rand_result.cv_results_['mean_test_score']\n",
    "stds = rand_result.cv_results_['std_test_score']\n",
    "params = rand_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (-mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the best configurarion on the test set.\n",
    "\n",
    "The best estimator can be obtained directly from the **model selection object**.\n",
    "\n",
    "**Moreover, the best estimator has been already retrained on the whole training set**.\n",
    "\n",
    "This option can be disabled settin the paramter **refit=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 5ms/step\n",
      "The MSE on the test set is 14.3577.\n",
      "The MAE on the test set is 2.6032.\n"
     ]
    }
   ],
   "source": [
    "best_model = rand_result.best_estimator_.model\n",
    "\n",
    "test_mse, test_mae = best_model.evaluate(x_test, y_test)\n",
    "\n",
    "print('The MSE on the test set is {:.4f}.\\n'\n",
    "      'The MAE on the test set is {:.4f}.'.format(test_mse, test_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
